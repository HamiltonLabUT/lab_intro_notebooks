{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09_mTRF_analysis\n",
    "\n",
    "**What is mTRF?**\n",
    "\n",
    "mTRF stands for \"multivariate temporal receptive field.\" It is an attempt to describe what stimuli excite or inhibit a neuron, or population of neurons, over time. It is developed from STRF which stands for \"spectrotemporal receptive field,\" expanded to work with all stimulus features, not just spectrotemporal ones.\n",
    "\n",
    "* m(ultivariate) = works with all stimulus types\n",
    "* T(emporal) = describes the response over time\n",
    "* R(eceptive) = describes how neurons are excited or inhibited\n",
    "* F(ield) = describes what stimuli excite or inhibit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some visual examples of mTRFs (technically this first one is a STRF, but STRFs are a subset of mTRFs):\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/75/Spectro-temporal_receptive_field_%28auditory_neuron_of_zebra_finch%29.jpg\"> </center>\n",
    "\n",
    "Here we see a neuron which is excited by frequencies in the 3-7Hz range at 10-11ms post stimulus presentation, then inhibited by that same frequency range 15-20ms post stimulus presentation. This pattern of activation is indicative of an \"edge detector\" neuron, one that likes rapid temporal changes in frequency.\n",
    "\n",
    "<center><img src=\"https://puu.sh/HPtgf/17480ffe3d.png\"></center>\n",
    "\n",
    "This is an example of a different feature space. This is a population of neurons (ECoG recording) that is excited and inhibited by different phonological features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The power of mTRF\n",
    "mTRF is an example of a subset of machine learning techniques called an _encoding model_. It is an attempt to predict neural activity based on stimulus information. This is the opposite of a _decoding model_ where you try to predict the stimulus information based on the neural activity.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "* [nVidia gaugan](http://nvidia-research-mingyuliu.com/gaugan)-- the stimulus features are the lines you draw, and the response is the image representation.\n",
    "* SIRI is a decoding model, which takes the response (audio) and tries to segment it into the stimulus features (words/questions).\n",
    "\n",
    "This is awesome because it gives an approach with a lot of computational and statistical power, and lets us ask questions about the structure of our data that we wouldn't be able to do with conventional statistical methods. However, it's complicated, and unless you're a statistician or computer scientist, chances are you didn't grow up learning how to conduct and interpret mTRF analysis (like me). The important thing to know about mTRF is that your **stimulus selection is very important, and in a lot of ways functions like the hypothesis of your experiment**. We can fit an infinite number of stimulus spaces to a single dataset, and it is our discretion as researchers to decide how to do this! This brings a lot of bias into the model in the sense that we will fit models that we think capture the structure of the data in some way. So just, be careful when selecting your stimulus features! (Also for this notebook we won't be tackling this problem for the sake of simplicity)\n",
    "\n",
    "For more information on the encoding/decoding framework, check out [this paper](https://www.frontiersin.org/articles/10.3389/fnsys.2017.00061/full)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model we will fit in this notebook\n",
    "We will be fitting a model using some EEG data from my thesis experiement, the same dataset we've been doing everything in the lab intro notebooks with.\n",
    "\n",
    "* The response: 64 channel EEG of a participant reading sentences and listening to playback of themselves reading those sentences.\n",
    "* The stimulus: three features: acoustic envelope (remember our audio preprocessing notebook?), then two binary features for consonant and vowel. That means, 1 if it's a cons or vowel, and 0 if it isn't. This gives us a total of 3 features (env, cons binary, vow binary) in our encoding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm, rcParams\n",
    "import re\n",
    "from supplemental_files import textgrid\n",
    "from supplemental_files.strf.strf import strf\n",
    "import csv\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import hilbert, butter, filtfilt, resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions\n",
    "def is_cons(transcript):\n",
    "    '''\n",
    "    Determine if a phonemic transcript is a consonant.\n",
    "    Returns:\n",
    "    * 0 if not consonant\n",
    "    * 1 if consonant\n",
    "    '''\n",
    "    if \"0\" in transcript: # vowels contain numbers to mark stress\n",
    "        return 0\n",
    "    elif \"1\" in transcript:\n",
    "        return 0\n",
    "    elif \"2\" in transcript:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def is_vow(transcript):\n",
    "    '''\n",
    "    Determine if a phonemic transcript is a vowel.\n",
    "    Returns:\n",
    "    * 0 if not vowel\n",
    "    * 1 if vowel\n",
    "    '''\n",
    "    if \"0\" in transcript: # vowels contain numbers to mark stress\n",
    "        return 1\n",
    "    elif \"1\" in transcript:\n",
    "        return 1\n",
    "    elif \"2\" in transcript:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Get the stimulus and the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw\n",
    "# REMEMBER: CHANGE THIS FPATH IF YOU ARE FOLLOWING ALONG AT HOME\n",
    "raw_fpath = 'F:/Desktop/example_preprocessed_data.fif'\n",
    "raw = mne.io.read_raw_fif(raw_fpath,preload=True,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load event file (this is how we get our C/V stimuli)\n",
    "sfreq = 128\n",
    "event_file = []\n",
    "ev_fpath = \"./supplemental_files/phoneme_event_file.txt\"\n",
    "with open(ev_fpath,'r') as csvfile:\n",
    "    csvReader = csv.reader(csvfile,delimiter=\"\\t\")\n",
    "    for row in csvReader:\n",
    "        onset = int(np.round(float(row[0])*sfreq))\n",
    "        offset = int(np.round(float(row[1])*sfreq))\n",
    "        transcript = row[3]\n",
    "        # These help prevent \"double dipping\" later on...\n",
    "        rep = int(row[4]) # how many times they've read a sentence\n",
    "        sn_id = int(row[5]) # which sentence the phoneme is from\n",
    "        event_file.append([onset,offset,transcript,rep,sn_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio envelope (this is how we get our env stimulus)\n",
    "# Code taken from audio_tools.py\n",
    "def get_envelope(audio, audio_fs, new_fs, cof=25, bef_aft=[0, 0], pad_next_pow2=False):\n",
    "    ''' Get the envelope of a sound file\n",
    "    Inputs:\n",
    "        w [float] : audio signal vector\n",
    "        fs [int] : sampling rate of audio signal\n",
    "        new_fs [int] : desired sampling rate of the envelope (same as your EEG, for example)\n",
    "    Outputs:\n",
    "        envelope [array-like] : returns the envelope of the sound as an array\n",
    "    '''\n",
    "    \n",
    "    if pad_next_pow2:\n",
    "        print(\"Padding the signal to the nearest power of two...this should speed things up\")\n",
    "        orig_len = len(audio)\n",
    "        sound_pad = np.hstack((audio, np.zeros((2**np.int(np.ceil(np.log2(len(audio))))-len(audio),))))\n",
    "        audio = sound_pad\n",
    "\n",
    "    print(\"calculating hilbert transform\")\n",
    "    env_hilb = np.abs(hilbert(audio))\n",
    "\n",
    "    nyq = audio_fs/2. #Nyquist frequency\n",
    "    b, a = butter(3, cof/nyq, 'low'); #this designs a 3-pole low-pass filter\n",
    "    \n",
    "    print(\"Low-pass filtering hilbert transform to get audio envelope\")\n",
    "    envelope_long = np.atleast_2d(filtfilt(b, a, env_hilb, axis=0)) #filtfilt makes it non-causal (fwd/backward)\n",
    "\n",
    "    envelope = resample(envelope_long.T, np.int(np.floor(envelope_long.shape[1]/(audio_fs/new_fs))))\n",
    "    if pad_next_pow2:\n",
    "        print(\"Removing padding\")\n",
    "        final_len = np.int((orig_len/audio_fs)*new_fs)\n",
    "        envelope = envelope[:final_len,:]\n",
    "        print(envelope.shape)\n",
    "\n",
    "    if bef_aft[0] < 0:\n",
    "        print(\"Adding %.2f seconds of silence before\"%bef_aft[0])\n",
    "        envelope = np.vstack(( np.zeros((np.int(np.abs(bef_aft[0])*new_fs), 1)), envelope ))\n",
    "    if bef_aft[1] > 0:\n",
    "        print(\"Adding %.2f seconds of silence after\"%bef_aft[1])\n",
    "        envelope = np.vstack(( envelope, np.zeros((np.int(bef_aft[1]*new_fs), 1)) ))\n",
    "\n",
    "    return envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get envelope\n",
    "# Change this path if you're following along at home\n",
    "# May take a while to run\n",
    "wav_fpath = 'F:/Desktop/full_block_audio.wav'\n",
    "sfreq, audio = wavfile.read(wav_fpath)\n",
    "env = get_envelope(audio, sfreq, 128)\n",
    "print(env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response matrix\n",
    "eeg_chs = mne.pick_types(raw.info,meg=False,eog=False,eeg=True)\n",
    "resp = raw.get_data(picks=eeg_chs).T\n",
    "n_samps = resp.shape[0]\n",
    "print(resp.shape) # samps x chs\n",
    "\n",
    "# Also, we need to clip the envelope so it's the same length as the response.\n",
    "# This is likely some random rounding error\n",
    "if env.shape[0] > resp.shape[0]:\n",
    "    env = env[:resp.shape[0]] # just cut off the last few samples\n",
    "    print(env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stimulus matrix\n",
    "features = [\"env\",\"cons\",\"vow\"]\n",
    "stim = np.zeros((n_samps,len(features))) \n",
    "for i, samp in enumerate(event_file):\n",
    "    onset = samp[0]\n",
    "    offset = samp[1]\n",
    "    c = is_cons(samp[2])\n",
    "    v = is_vow(samp[2])\n",
    "    e = env[i]\n",
    "    rep = samp[3]\n",
    "    sn_id = samp[4]\n",
    "#     stim[onset,0] = e\n",
    "    stim[onset,1] = c\n",
    "    stim[onset,2] = v\n",
    "stim[:,0] = env[:,0]/env[:,0].max() # normalize envelope so the max is 1\n",
    "print(stim.shape) # samps x features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression schematic\n",
    "# Plot stim & resp\n",
    "fig, axes = plt.subplots(figsize=(16,16)) # make a figure of size 12 x 8\n",
    "\n",
    "# Subplot 1: show binary features (stimulus)\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "t_start = np.int(2500*128)\n",
    "t_end = np.int(2515*128)\n",
    "plt.imshow(stim[t_start:t_end,:].T, aspect='auto',interpolation=\"nearest\",cmap=cm.Reds);\n",
    "plt.gca().set_yticks(np.arange(3))\n",
    "plt.gca().set_yticklabels(features)\n",
    "plt.title('Phonological features')\n",
    "#plt.gca().set_ylim([-0.5, 28.5])\n",
    " \n",
    "# Subplot 2: show the EEG (response)\n",
    "plt.subplot(2,1,2)\n",
    "zs = lambda x: (x-x.mean(0))/x.std(0) # z scoring it\n",
    "zs_resp = zs(resp)\n",
    "rmax = np.max(np.abs(zs_resp))*0.1\n",
    "plt.imshow(zs_resp[t_start:t_end,:].T, vmin=-rmax, vmax=rmax, cmap = cm.RdBu_r, aspect='auto', interpolation = 'nearest') \n",
    "plt.ylabel('Electrode')\n",
    "plt.xlabel('Time bin')\n",
    "plt.gca().set_yticks(np.arange(len(raw.info['ch_names'])))\n",
    "plt.gca().set_yticklabels(raw.info['ch_names'],fontsize=6)\n",
    "plt.title('Neural response')\n",
    "fig.subplots_adjust(hspace=.5) # Put some space between the plots for ease of viewing\n",
    "# Set some parameters for this and future plots\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['text.usetex'] = False\n",
    "rcParams['axes.labelsize'] = 10\n",
    "rcParams['xtick.labelsize'] = 10\n",
    "rcParams['ytick.labelsize'] = 10\n",
    "rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Subplot 3: Regression schematic\n",
    "fig = plt.figure(figsize=(17,5))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.imshow(stim[t_start:t_end,:].T, cmap = cm.Reds, aspect='auto')\n",
    "plt.xlabel('Time bin')\n",
    "ax1.set_ylim(ax.get_ylim()[::-1]) # This just reverses the y axis so low frequency is at the bottom\n",
    "ax1.set_ylabel('Feature')\n",
    "ax1.set_yticks(np.arange(len(features)))\n",
    "ax1.set_yticklabels(features)\n",
    "ax1.set_xlim(0,t_end-t_start)\n",
    "# Plot the response overlayed (with a separate y axis scaled appropriately)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(zs_resp[t_start:t_end,:].mean(1), 'b') \n",
    "ax2.set_ylabel('Z scored response', color='k')\n",
    "for tl in ax2.get_yticklabels():\n",
    "    tl.set_color('k')\n",
    "ax2.set_xlim(0,t_end-t_start);\n",
    "ax2.set_ylim(-6, 4)\n",
    "plt.title('Regression schematic');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Split stim and resp into training and validation sets\n",
    "In this example, we have 100 sentences in the dataset. We will train on 80 of these sentences then test our model on the remaining 20 sentences. This code splits those 100 sentences into train/validation sets, paying special attention to not include one sentence in both sets, because that would be \"cheating\" the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have stim and resp we need to split them into training and validation sets.\n",
    "            \n",
    "# Step 1: read sentence event file\n",
    "sen_onset,sen_offset,sen_id = [],[],[]\n",
    "ev_fname = './supplemental_files/sentence_event_file.txt'\n",
    "with open(ev_fname,'r') as my_csv:\n",
    "    csvReader = csv.reader(my_csv,delimiter='\\t')\n",
    "    for row in csvReader:\n",
    "        sen_onset.append(float(row[0]))\n",
    "        sen_offset.append(float(row[1]))\n",
    "        sen_id.append(int(row[2]))\n",
    "        \n",
    "# Step 2: Make a dict split by unique ID so we don't double dip IDs when splitting\n",
    "# into training and validation sets\n",
    "sen_samps = dict() # sample ranges for each unique ID\n",
    "for unique_idx, unique_id in enumerate(np.unique(sen_id)):\n",
    "    sen_range = []\n",
    "    for this_idx, this_id in enumerate(sen_id):\n",
    "        if this_id == unique_id: # We got a match!\n",
    "            onset_samp = int(sen_onset[this_idx]*128)\n",
    "            offset_samp = int(sen_offset[this_idx]*128)\n",
    "            sen_range.append([onset_samp,offset_samp])\n",
    "    sen_samps[unique_id] = sen_range\n",
    "    \n",
    "# Step 3: match each sentence to the stims and resps arrays created earlier\n",
    "jitter = 0.5\n",
    "resp_dict,stim_dict = dict(),dict()\n",
    "for unique_id in sen_samps.keys():\n",
    "    resp_matches, stim_matches = [], []\n",
    "    for unique_idx, times in enumerate(sen_samps[unique_id]):\n",
    "        onset_samp, offset_samp = times[0], times[1]\n",
    "        for samp_idx in range(len(resp)):\n",
    "            if samp_idx >= onset_samp-jitter and samp_idx <= offset_samp+jitter:\n",
    "                resp_matches.append(resp[samp_idx,:])\n",
    "                stim_matches.append(stim[samp_idx,:])\n",
    "    resp_dict[unique_id] = np.array(resp_matches) # samps x chs\n",
    "    stim_dict[unique_id] = np.array(stim_matches) # samps x feats\n",
    "\n",
    "# Step 4: split stim and resp dicts into training and validation sets\n",
    "np.random.seed(6655321)\n",
    "train_sen_ids = np.random.permutation(list(resp_dict.keys()))[:80]\n",
    "np.random.seed(6655321)\n",
    "val_sen_ids = np.random.permutation(list(resp_dict.keys()))[80:]\n",
    "tStims_by_sen,vStims_by_sen,tResps_by_sen,vResps_by_sen = dict(),dict(),dict(),dict()\n",
    "# It needs to match with the true sentence id, not where it is in a sequence of 20\n",
    "# There has to be some clever way to do this... try on Monday or Tuesday\n",
    "for train_id in train_sen_ids:\n",
    "    tResps_by_sen[train_id] = resp_dict[train_id]\n",
    "    tStims_by_sen[train_id] = stim_dict[train_id]\n",
    "for val_id in val_sen_ids:\n",
    "    vResps_by_sen[val_id] = resp_dict[val_id]\n",
    "    vStims_by_sen[val_id] = stim_dict[val_id]\n",
    "tStim = np.vstack(list(tStims_by_sen.values()))\n",
    "vStim = np.vstack(list(vStims_by_sen.values()))\n",
    "tResp = np.vstack(list(tResps_by_sen.values()))\n",
    "vResp = np.vstack(list(vResps_by_sen.values()))\n",
    "print(f\"Training on {tStim.shape[0]} samples, validating on {vStim.shape[0]} samples. raw contained {resp.shape[0]} samples,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the mTRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delay matrices\n",
    "# mTRFs run a series of linear regressions on timeshifted (delayed) data, in order to get the \"temporal\" aspect of the TRF\n",
    "# It's the timelag between the stimulus happening and the corresponded response.\n",
    "# Some people conceptualize this as system \"memory\"\n",
    "# When selecting delays, you want to incorporate a time range that you theorize your stimuli will elicit a response within\n",
    "# In this example, we will see responses 300ms before a phoneme and up to 500ms after a phoneme.\n",
    "delay_min, delay_max = -0.3, 0.5 # The window of interest (in s)\n",
    "# Create a numpy array that is a range of steps in this time window\n",
    "delays = np.arange(np.floor(delay_min*128),np.ceil(delay_max*128),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set number of bootstraps (number of iterations in the strf)\n",
    "# Basically, run your sTRF this amount of times. If it looks the same each time then your model is stable.\n",
    "nboots = 10\n",
    "# Create range of alphas - a fixed parameter in the mTRF that can affect the performance. you may have to play around with it\n",
    "# More specifically, it's a regularization parameter. The larger the alpha, the smoother your weights will be.\n",
    "alphas = np.hstack((0,np.logspace(1,5,20)))\n",
    "# To run the strf, we call a function from supplemental_files/strf.py. Look at this script if you're curious how it works!\n",
    "corrs, wts, tStim, tResp, vStim, vResp, alphas, pred = strf(tResp, tStim, nboots=nboots, delay_min=delay_min, delay_max=delay_max,\n",
    "                                                     vResp=vResp, vStim=vStim, flip_resp=True, alphas=alphas, sfreq=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Tweak your mTRF parameters (REWRITE THIS)\n",
    "There are several (in my opinion) unintuitive aspects of running a mTRF. These are your hyperparameters:\n",
    "\n",
    "* `nboots` -- the number of times to run your mTRF. This is mostly bottlenecked by your computer's performance, how many \"laps\" can it \"run?\" 10 is a good start but if you are having troubles try going lower. Try going to 20 if you think you are big and tough. The number of bootstraps also tells you how stable your model is -- if you are getting similar results with each run, then your model is stable. Yay!\n",
    "* `alphas` -- this one is tricky. The alpha is a regularization parameter and acts to sort of smooth (or \"regularize,\" i guess) your data. Your best-performing alpha should be right in the middle of your selected logspace. If it's too close to one side of the logarithmic curve, you are either overfitting or underfitting your mTRF. A low alpha will result in pixelated receptive fields while a high alpha will result in over-smoothed receptive fields. \n",
    "<center><img src=\"https://puu.sh/HPKgv/084fe16de2.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha selection\n",
    "plt.figure(figsize=(5,5))\n",
    "best_alpha = alphas[0]\n",
    "print(best_alpha)\n",
    "plt.semilogy(np.logspace(1,5,20));\n",
    "plt.axhline(best_alpha,color='r',label=\"best alpha (%.2f)\" % best_alpha);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate your mTRF performance\n",
    "There are many ways to do this. Here are a few:\n",
    "\n",
    "* Look at your predicted vs. actual response. This is a direct measure of how well your model performed, but may not be super informative.\n",
    "* Look at the contributions of individual weights. This shows you how your feature selection informed your model performance. Remember, the features you select are your hypothesis, so this is a potentially very useful analysis!\n",
    "* Look at how well your model did versus random chance. To do this, you can perform a bootstrap task. This is nice for getting statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start,t_end = 0, 1280\n",
    "plt.figure(figsize=(8,6))\n",
    "# Predicted versus actual response for a single channel\n",
    "plt.subplot(2,1,1)\n",
    "chan = corrs[0].argmax() # Best correlated channel\n",
    "plt.plot(vResp[t_start:t_end,chan]);\n",
    "plt.plot(pred[t_start:t_end,chan])\n",
    "plt.title(f\"Best correlated channel {raw.info['ch_names'][chan]} (r={corrs[0][chan]})\", fontsize=12);\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "chan = corrs[0].argmin() # Worst correlated channel\n",
    "plt.plot(vResp[t_start:t_end,chan]);\n",
    "plt.plot(pred[t_start:t_end,chan])\n",
    "plt.title(f\"Worst correlated channel {raw.info['ch_names'][chan]} (r={corrs[0][chan]})\",fontsize=12);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(corrs);\n",
    "plt.axvline(0,color='k')\n",
    "plt.title(\"Histogram showing correlation of predicted with actual response for individual channels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape weights to delays x feats x chans\n",
    "rwts = wts[0].reshape(-1,stim.shape[1],resp.shape[1])\n",
    "# Plot weights by channel\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "wtmax = np.abs(rwts).max()*0.6\n",
    "for i in np.arange(resp.shape[1]): # Loop through each channel\n",
    "    ch_name = raw.info['ch_names'][i]\n",
    "    plt.subplot(8,8,i+1)\n",
    "    plt.imshow((rwts[:,:,i]).T, vmin=-wtmax, vmax=wtmax, aspect=\"auto\", cmap=cm.RdBu_r, interpolation=\"nearest\")\n",
    "    plt.title(\"%s (r=%.2f)\" % (ch_name, corrs[0][i]))\n",
    "    if i == 56: # Bottom left\n",
    "        plt.gca().set_yticks(np.arange(rwts.shape[1]))\n",
    "        plt.gca().set_yticklabels(features)\n",
    "        plt.gca().set_xticks([0,128*-delay_min,len(delays)])\n",
    "        plt.gca().set_xticklabels([delay_min,0,delay_max]) \n",
    "    else: # Don't label the axes for the other plots, just to save space since theyre all the same ya know\n",
    "        plt.gca().set_yticks([])\n",
    "        plt.gca().set_xticks([])\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.axvline(128*-delay_min,color='k')\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for significance\n",
    "\n",
    "This notebook is already pretty long, so I've left this out! A simple example of how to do this can be found in my thesis code [here](https://github.com/HamiltonLabUT/onsetProd/blob/master/ecog/strf/bootstrapping.ipynb) -- look at cell 4.\n",
    "\n",
    ":)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
